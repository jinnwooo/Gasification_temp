import torch
import numpy as np
from tqdm import tqdm
import glob
import os
import shap

def train_model(model, train_loader, criterion, optimizer, device, epoch, writer=None):
    model.train()
    running_loss = 0.0

    # tqdmìœ¼ë¡œ ê°ì‹¸ê¸°: descì— epoch ì •ë³´ í‘œì‹œ
    train_loader_tqdm = tqdm(train_loader, desc=f"[Train] Epoch {epoch+1}", unit="batch", dynamic_ncols=True, leave=True)

    for batch_idx, (X_seq, y) in enumerate(train_loader_tqdm):
        X_seq, y = X_seq.to(device), y.to(device)

        optimizer.zero_grad()
        outputs = model(X_seq)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # tqdmì— í˜„ì¬ Lossë¥¼ í‘œì‹œ
        train_loader_tqdm.set_postfix(loss=f"{loss.item():.4f}")

    epoch_loss = running_loss / len(train_loader)

    # TensorBoard ê¸°ë¡ (train loss)
    if writer:
        writer.add_scalar('Loss/Train', epoch_loss, epoch)

    return epoch_loss


def evaluate_model(model, loader, criterion, device, epoch, writer=None, phase="Val"):
    model.eval()
    running_loss = 0.0

    # ì˜ˆì¸¡/ì‹¤ì œê°’ ì €ì¥
    all_preds, all_trues = [], []

    # tqdm: ì§„í–‰ ìƒí™© í‘œì‹œ
    loader_tqdm = tqdm(loader, desc=f"[{phase}] Epoch {epoch}", unit="batch", dynamic_ncols=True, leave=True)

    with torch.no_grad():
        for batch_idx, (X_seq, y) in enumerate(loader_tqdm):
            X_seq, y = X_seq.to(device), y.to(device)
            outputs = model(X_seq)
            loss = criterion(outputs, y)
            running_loss += loss.item()

            all_preds.append(outputs.cpu().numpy())  # (batch, 4)
            all_trues.append(y.cpu().numpy())  # (batch, 4)

            # tqdmì— í˜„ì¬ Lossë¥¼ í‘œì‹œ
            loader_tqdm.set_postfix(loss=f"{loss.item():.4f}")

    loss = running_loss / len(loader)

    # TensorBoard ê¸°ë¡
    if writer:
        writer.add_scalar(f'Loss/{phase}', loss, epoch)

    # ë°°ì—´ ì—°ê²°
    all_preds = np.concatenate(all_preds, axis=0)  # shape: (N, 4)
    all_trues = np.concatenate(all_trues, axis=0)  # shape: (N, 4)

    return loss, all_trues, all_preds

def get_csv_files(group_path):
    return glob.glob(os.path.join(group_path, '*.csv'))


def determine_sample_size(train_loader, max_sample_size=2000):
    """
    Train ë°ì´í„° í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SHAP ìƒ˜í”Œë§ ê°œìˆ˜ë¥¼ ìë™ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    :param train_loader: PyTorch DataLoader (Train ë°ì´í„°)
    :param max_sample_size: ìµœëŒ€ ìƒ˜í”Œë§ ê°œìˆ˜ (ê¸°ë³¸ê°’: 2000)
    :return: ì‚¬ìš©í•  ìƒ˜í”Œë§ ê°œìˆ˜
    """
    train_data_size = len(train_loader.dataset)

    # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ìƒ˜í”Œë§ ìˆ˜ ê²°ì •
    if train_data_size <= 5000:
        return train_data_size  # ì „ì²´ ë°ì´í„° ì‚¬ìš©
    elif train_data_size <= 50000:
        return min(2000, train_data_size)  # ìµœëŒ€ 2000ê°œ ìƒ˜í”Œ
    else:
        return min(max_sample_size, train_data_size)  # ìµœëŒ€ max_sample_size ê°œ ìƒ˜í”Œ


def calculate_shap_values(model, loader, device):
    """
    RNN/LSTM/GRU ëª¨ë¸ì—ì„œ SHAP ê°’ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ (requires_grad ë¬¸ì œ í•´ê²°)
    :param model: í•™ìŠµëœ ëª¨ë¸
    :param loader: Train ë°ì´í„°ì…‹ì˜ DataLoader
    :param device: CPU or CUDA
    :return: SHAP values, ìƒ˜í”Œ ë°ì´í„°
    """
    model.train()  # ğŸ”¹ SHAP ê³„ì‚°ì„ ìœ„í•´ train ëª¨ë“œ ìœ ì§€
    torch.backends.cudnn.enabled = False  # ğŸ”¹ CuDNN ë¹„í™œì„±í™” (backward ë¬¸ì œ ë°©ì§€)

    sample_size = determine_sample_size(loader)  # ìë™ ìƒ˜í”Œë§ ê°œìˆ˜ ê²°ì •

    # ì „ì²´ train_loaderì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    all_data = []

    for X_batch, _ in tqdm(loader, desc="Loading Train Data for SHAP", leave=True):
        all_data.append(X_batch)

    X_all = torch.cat(all_data, dim=0)  # ëª¨ë“  ë°°ì¹˜ë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ í•©ì¹˜ê¸°

    # ìƒ˜í”Œ í¬ê¸° ì œí•œ (ëœë¤ ìƒ˜í”Œ ì„ íƒ)
    if X_all.shape[0] > sample_size:
        idx = np.random.choice(X_all.shape[0], sample_size, replace=False)
        X_sample = X_all[idx]
    else:
        X_sample = X_all

    X_sample = X_sample.to(device)

    # ğŸ”¹ SHAP ê³„ì‚°ì„ ìœ„í•´ requires_grad í™œì„±í™”
    X_sample.requires_grad = True

    # SHAP Explainer ìƒì„± (GradientExplainer ì‚¬ìš©)
    explainer = shap.GradientExplainer(model, X_sample)

    # tqdm ì ìš©í•˜ì—¬ SHAP ê°’ ê³„ì‚° ì§„í–‰ë¥  í‘œì‹œ
    with tqdm(total=len(X_sample), desc="Computing SHAP Values", leave=True) as pbar:
        for i, x in enumerate(X_sample):
            shap_values = explainer.shap_values(x.unsqueeze(0))  # ê°œë³„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ SHAP ê³„ì‚°
            pbar.update(1)  # tqdm ì—…ë°ì´íŠ¸

    # ğŸ”¹ SHAP ê³„ì‚° í›„ requires_grad ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)
    X_sample.requires_grad = False

    model.eval()  # ğŸ”¹ SHAP ê³„ì‚°ì´ ëë‚œ í›„ ë‹¤ì‹œ eval() ëª¨ë“œë¡œ ë³€ê²½
    torch.backends.cudnn.enabled = True  # ğŸ”¹ CuDNN ë‹¤ì‹œ í™œì„±í™”

    return shap_values, X_sample.cpu().numpy()
